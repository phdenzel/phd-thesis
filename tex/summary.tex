%% summary.tex

The previous sections introduced gravitational lensing and related topics in a
general context.  Here, my own contributions\sidenote{with the help and guidance
of colleagues listed in the beginning of each chapter.} to these fields, in
particular strong gravitational lensing are introduced and summarised.

\secref{lensing} describes how the study of the arrival-time surface explains
the occurrence of multiple images; see \eqref{arrival_time_surface}.  Extremal
points where $\nabla\tau = 0$, are the only positions where the arrival-time
surface is observable.  Thus, any mass distribution $\kappa$ which preserves
these points up to a constant, is a valid solution to the lensing equation
\eqref*{lens_equation}.  In fact, lensing is subject to various forms of
degeneracies which cause the system to have even infinitely many solutions.  An
especially troublesome form is the steepness or mass-sheet degeneracy, which
becomes apparent if \eqref{arrival_time_surface} is slightly rewritten as
%
\begin{equation}\eqlbl{tau_degen}%
    \tau(\bm\theta) = 2\nabla^{-2}(1 - \kappa) - \bm\theta\cdot\bm\beta
\end{equation}%
%
where the $\frac{1}{2}\beta^2$ was discarded, because it is constant for all
$\bm\theta$.  In this form it is evident that scalar transformations,
corresponding to replacing the terms with $(1-\kappa) = \lambda(1-\kappa')$ and
$\bm\beta = \lambda\bm\beta'$, rescale the entire arrival-time surface by an
arbitrary factor $\lambda$ and change its shape, but leave the image structure
and their relative magnifications invariant.

The scarceness of lensing observables and the resulting degeneracies are
arguably the greatest challenge for modelling lenses.  Some modelling strategies
respond to this problem by solving for many solutions to the linear problem
defined by
%
\begin{equation}\eqlbl{fermat}%
    \nabla\tau(\bm\theta) = 0.
\end{equation}%
%
\Code{GLASS}, a modern free-form modelling framework by \sideciteay{GLASS}, is
especially efficient in solving for large ensembles of models for this problem.
However, despite constraining its solution space with physical and
regularization priors, many mass distributions built by this free-form modeller
do not describe realistic galaxies.  \chref{delays} and \chref*{fossil}
(summarised in \subsecref{delays} and \subsecref*{fossil}) present lens models
which were generated by \Code{GLASS}, but further constrained with specific
physical ancillary information about the lensing galaxy in order to obtain more
realistic mass-distribution models.  

In \chref{delays} (and \subsecref{delays}), we used ensembles of free-from models of
time-delay lenses to infer the Hubble constant.  Each model consisted of 8
lenses for which time delays, that is differences in arrival-times between
lensed images $\tau(\bm\theta_{i}) - \tau(\bm\theta_{j})$, were required to fit
the measured data.  We confirmed that time delays are extremely effective in
constraining the mass distribution of lensing galaxies, and help produce more
realistic models.

In \chref{fossil} (and \subsecref{fossil}), we combined a free-form lensing
analysis with stellar mass models to reduce degeneracies in the inner region of
a very massive galaxy, the lens of SW05 (see \figref{moster}).  The photometry
of the lensing galaxy was modelled and combined with light-to-stellar mass
estimates obtained through marginalization over various stellar population
synthesis models.  Subtracting the stellar mass from the lens mass distribution
furthermore yielded a high-resolution dark matter map of the galaxy's halo.

Both of these works presented highly constrained state-of-the-art lens models,
which accurately reproduced lens image positions, but nonetheless exhibited a
substantial scatter in their lens mass distributions and uncertainty in derived
results due to the steepness degeneracy.  The steepness-degeneracy
transformations motivate the definition of another quantity which is introduced
in \chref{fossil}, the \textit{lensing Roche potential}
%
\begin{equation}\eqlbl{roche_potential}%
    \begin{aligned}%
        \mathcal{P}(\bm\theta) = 
        \frac{1}{2}\theta^{2} - 2\nabla^{-2}\kappa(\bm\theta) \\
        \tau(\bm\theta) = \mathcal{P}(\bm\theta) - \bm\beta\cdot\bm\theta.
    \end{aligned}%
\end{equation}%
%
The advantage of the lensing Roche potential is that unlike the arrival-time
surface, steepness-degeneracy transformations leave its shape invariant.  This
is especially useful in comparisons of different model $\kappa$, as variations
due to changing levels of steepness degeneracy can be isolated through their
lensing Roche potentials, i.e. $\mathcal{P}$ can be normalised without problems
and then compared using scalar products.

Applying Fermat's principle to the lensing Roche potential by inserting
\eqref*{roche_potential} into \eqref{fermat} yields
%
\begin{equation}%
    \bm\beta = \nabla\mathcal{P}(\bm\theta)
\end{equation}%
%
which describes how source positions are mapped onto extended images; also see
\eqref{lens_equation}.  These provide more constraints than individual
point-image positions, but represent non-linear priors in $\kappa$ and thus can
not be efficiently integrated to free-form lensing frameworks such as
\Code{GLASS}.  

For this reason, I developed an analytical framework \Code{gleam} \cite{gleam}
which includes post-processing filters for ensembles of arbitrary mass
distributions.  It provides extended-image least-squares fitting diagnostics,
such as entire source-plane and synthetic lens-plane reconstructions (synthetics
for short).  In \chref{adler} (and \subsecref{adler}), it was blind-tested
against 15 synthetic lenses from hydrodynamical simulations.  We ran the
diagnostics on ensembles of free-form models and filtered for models which
concur with the extended-image data.  The final results of the tests were
sobering, because they pointed out the gravity of the steepness degeneracy
problem, even after highly constraining the models with extended images.  In
fact, that originally motivated the introduction of the lensing Roche potential
and a method to isolate the degeneracy in a comparison.

All previous works inevitably demonstrated the necessity to employ much more
intricate galaxy models than is conventionally done.  Correspondingly, the study
in \chref{match} (and \subsecref{match}) gives proof-of-concept for a novel
'modelling-free' approach in which galaxies from hydrodynamical simulations are
directly matched to lensing observations using the \Code{gleam} framework.  The
proposed methodology promises great potential as it is straight-forward to scale
up to big data sets.  Moreover, we propose several ways to extend the
methodology to further include common lensing constraints in order to test
various galaxy-formation scenarios against each other.

In the following subsections, each of my studies mentioned above and detailed in
the remaining chapters is described and summarised in the same order.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inferring the Hubble constant}\subseclbl{delays}

\chref{delays} presents an estimate of the Hubble constant
using time delays of previously well-studied systems, with a critical study of the uncertainties.  With an analysis on 8
time-delay lenses, we determined the Hubble constant with a value of
%
\begin{equation*}%
    \Ho = 71.8^{+3.8}_{-3.3}\;\Hunitsalt
\end{equation*}%
%
Contrary to other studies of the same systems, our inferred value is compatible
with both the early and late measurements, with a tension of less than
$1.5\sigma$.  With a precision of 5\%, the measurement is unfortunately not
able to contribute to the resolution of the Hubble tension.  In fact, the
investigation revealed that, if a 1\% level is at all obtainable, it would
require a joint-analysis of many more time-delay measurements of quadruply
imaging lens systems (quads).  Unfortunately, it cannot be ruled out that
time-delay galaxy lenses naturally exhibit an inherent scatter which prevents a
precise determination of the Hubble constant at a 1\% level.  

It was not the only \Ho{} determination through lensing observations recently;
the H0LiCOW collaboration \sidecite{Wong20} reported on a Hubble constant of
$73.3^{+1.7}_{-1.8}\;\Hunitsalt$ from 6 time-delay lenses a year before, and the
STRIDES collaboration \sidecite{Shajib20} determined a value of
$74.2^{+2.7}_{-3.0}\;\Hunitsalt$ from a single lens.  However, while those
works assumed parametric forms for the lenses, our study explores many different
lens models and attempts to marginalize over lensing degeneracies.
Although degeneracies are often neglected in lensing studies, it is known for a
long time that a single family of lens mass distributions is able reproduce the
same observables, but still yield different values for the Hubble constant.
Therefore, studies with the aim of inferring the Hubble constant need to solve
for (ideally) all possible solutions of mass distributions in order to retrieve
a complete model.  The study produced 8000 lens mass distributions with 1000
values for the Hubble constant.  Most interestingly, the values appeared to be
asymmetrically distributed, which might indicate that the errors on \Ho{} could
be non-Gaussian.  

These results were not entirely unexpected since the investigation was actually
a continuation of ideas gathered during a participation in a scientific blind
study \sidecite{TDLMC2} in which 50 simulated time-delay lenses were analysed by
several research groups.  It discovered that most other current lens recovery
methods are accurate to only about 6\%, even with acclaimed precision over
nearly 1\%.  In comparison with the study presented in \chref{delays}, it became
apparent that the lens simulations considerably differed from real observations
and that the uncertainties in the inference of \Ho{} from mock lenses were
higher.  Besides the obvious numerical deviations, the radial distribution of
lens images of the simulated lens set was relatively narrow which provided only
little constraints on the slope of the galaxy-density profiles and consequently
on \Ho.  This could mean that there exists a limit to the accuracy on \Ho{}
achievable with time-delay galaxy lenses, which ultimately might preclude them
to infer \Ho{} on a level required to resolve the Hubble tension.  Nonetheless,
gravitational lenses are excellent cosmological probes. Especially cluster
lenses do not exhibit this limitation and might still recover the Hubble
constant with sufficient precision to contribute to the resolution of the Hubble
tension.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lens recovery of SW05}\subseclbl{fossil}

In \chref{fossil}, we report a lens model of a special gravitational lens
J143454.4+522850 (SW05).  It was discovered by the Space Warps citizen science
campaign, and initially modelled by a smaller group of volunteers under the lead
of \sideciteay{Kueng18}.  From the initial investigation, it became apparent
that follow-ups of SW05 would yield interesting results as its mean radial image
separation at 5\,\arcsec is unusually high for a lensing galaxy which would put
it in a mass range close to $\sim10^{13}\,\Msol$.

Therefore, we free-form modelled the lens mass dsitribution again using
\Code{GLASS} with similar results.  However, there were several issues which
have been improved upon the initial model.  For one, the redshift estimates for
the lens and the source were previously based on multi-band colours alone, which
comes with high uncertainties.  Thus, we used the Bayesian photometric redshift
estimator \Code{bpz} by \sideciteay{bpz} to obtain preciser results for the lens
redshift of $z_{l} = 0.63$. The source redshift had since been measured
spectroscopically to $z_{s} = 2.96$.

Furthermore, the photometry of the lensing galaxy was fitted using a S\'ersic
surface profile\sidenote{an ellipticals surface brightness profile}.
Afterwards, this light model was fitted to spectral templates of 12
stellar-population base models \sidecite{BruzualCharlot03}.
% \begin{figure}[h]
%    \centering%
%    \includegraphics[width=0.99\textwidth]{stellarpop_spectra}%
%    \caption[]{}%
%    \figlbl{stellar_pops}%
% \end{figure}
These were marginalised over formation epoch, star-formation time scale, and
stellar metallicities; the 12 base models had 3 different stellar metallicities,
and 4 different age ranges over which the star formation was assumed constant.
Integration of the stellar populations with a Chabrier Initial Mass Function
\sidecite{ChabrierIMF}, yielded a relation of the luminosity of galaxies to
their stellar masses; hereby, the extinction by dust which generally reddens
their spectra had to be accounted for.  This resulted in a better and improved
stellar mass estimate and a stellar mass distribution for the lens of SW05.
Final mass estimates (lower limits) for the lensing galaxy of SW05 were
$(1.42\pm0.28)\cdot10^{13}\,\Msol$ for the total mass, and
$(3.04\pm0.22)\cdot10^{11}\,\Msol$ for the total mass in stars.  This is
consistent with an early-type elliptical galaxy with a mass in the galaxy group
range.

Lens models inherently account for all mass components of the galaxy. This means
if the stellar mass distribution of a galaxy is known, it can be disentangled
from the lens mass map and an estimate for the dark matter
distribution\sidenote{as long as the gas mass of the lensing galaxy is
negligible} remains.  Likewise, we presented a high-resolution dark matter map
for the lensing galaxy of SW05.  Moreover, the analysis allowed to reconstruct
the kinematics of the galaxy, which suggested the probability of missing mass in
form of hot intra-galactic gas distributed around the outskirts of the galaxy.
This is indicative of a specific type of elliptical galaxy, a \textit{fossil
group galaxy}.  To confirm these suspicions, the results were finally compared
to state-of-the-art hydrodynamical simulations from FIRE \sidecite[Feedback In
Realistic Environments;]{FIRE}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Models of simulations}\subseclbl{adler}

Free-form lens models like the ones produced by \Code{GLASS} can generally have
almost any shape\sidenote{especially further away from the radial band around
the images} and still fit point-images in observations.  So, the only choice is
to generate as many solutions as possible that are consistent with the data, and
try to find statistical tendencies which might indicate likely features a lens
might have.  Thereby, a lens investigator can be tempted to introduce personal
preferences and confirmation biases into the modelling process.  Thus, we
performed a blind-study on simulated lenses in order to equitably test our
free-form approach. \chref{adler} reports the results of this study.  

In total, 15 galaxies from the EAGLE (Evolution and Assembly of GaLaxies and
their Environment) suite of hydrodynamical simulations were projected onto the
lens plane and raytraced using the \Code{SEAGLE} pipeline.  The resulting lens
systems mock SDSS (Sloan Digital Sky Survey) observations. They feature
prominent, extended images and arcs, and their lensing galaxies incorporate
galaxy formation schemes which were shown to be in good agreement with
observations with respect to star formation rates, total stellar luminosity and
colours, and evolution of the galaxy stellar mass function and sizes.

In the initial phase of the blind-study, these simulated lens systems were
free-form modelled such that they conformed with image-point positions in the
data.  After some diagnostics to ensure the overall validity of the ensembles,
we computed synthetics (with fitted sources) for each of the 15'000 mass models
along with least squares corresponding to their fitting quality.  Including the
entire extended image data in the synthetics demonstrated that many ensemble
models agree with the image positions, but fail to reproduce more subtle
features of the lensed images.  Thus, such models were filtered out of the
ensembles, which elevated the overall quality of the lens models considerably.

After the subsequent unblinding of the true convergences, the shape,
compactness, and ellipticity of the mass models were analysed and compared to
the originals.  The modelled Einstein radii\sidenote{the average radius of the
area of critical density; see the paragraph after \eqref{convergence} for
details.} matched very accurately in comparison to the true mass distributions,
which was generally expected, as they are closely tied to main modelling input
that is the radial positions of the observed images. Correspondingly, the rough
shapes of the models also agreed well in the comparison.  However, the
ellipticity of the modelled mass distributions were systematically too low, and
their circularly averaged profiles similarly were too shallow.  This is a
typical manifestation of the steepness degeneracy, which means that even when
models are tightly constrained with extended image data, there are still
multiple convergence maps which reproduce the observations.

Since the study used simulations for which the mass distributions were known a
priori, the modelled mass maps corresponding to the best fitting results can in
principle be found by comparison; however, the steepness degeneracy makes direct
comparisons problematic.  Thus, we introduced a novel method to directly compare
shapes of convergence maps through their lensing Roche potentials (see
\eqrefp{roche_potential}) for which the steepness degeneracy is isolated.
Specifically, the similarity of two mass distributions can be evaluated with the
scalar product of their normalised lensing Roche potentials.  With the steepness
degeneracy finally factored out, the models agreed relatively well with the true
convergences on average, but were generally too round.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The lens-matching method}\subseclbl{match}

Lens modelling techniques conventionally build lenses based on recipes which aim
to efficiently reproduce shapes and slopes of galaxies, as they are usually
observed.  Such methods therefore suppress or even completely ignore the
evolutionary processes of galaxies and the physical properties which form and
drive them.  In contrast, cosmological hydrodynamical simulations have
progressed remarkably in recent years and employ semi-analytical models which
imitate star formation processes and thereof resultant feedback effects on
smallest scales to play through various galaxy-formation scenarios.

\chref{match} reports on a proof-of-concept study which proposes an alternative
strategy, the \textit{plausible lens match} method, to recover lenses and
explain observations.  It is demonstrated with a catalogue of 554 galaxies from
the cosmological hydrodynamical simulation suite EAGLE which were projected onto
differently oriented convergence maps using the \Code{SEAGLE} pipeline.  
Galaxies from two different formation scenarios were included in the catalogue.
The in total 1662 convergence maps\sidenote{for each galaxy three axial
projections were performed} are then directly matched to 7 SLACS (Sloan Lens
Advanced Camera System) observations using the \Code{gleam} framework
\cite{gleam}. 

The matching process consisted of three main steps: (i) First, the convergence
maps were rescaled to the appropriate redshifts of the lens observations, (ii)
then a range of maps with plausible Einstein radii were pre-selected, (iii) and
finally optimal alignments between the convergence maps and the observations
were determined based on the goodness of fit of their synthetic images.  Amongst
all maps from the catalogue, the ones with reduced $\chi^{2}$ from the synthetic
fitting close to 1 can be considered plausible matches which are consistent with
the entire extended image data.

While it was generally thought impossible to find matching galaxies from
simulations, the method found plausible matches for all 7 test cases.  The fact
that mass reconstructions of lensing galaxies is non-unique makes this possible.
The plausible lens match method shows great potential as it can easily be scaled
up to efficiently match thousands of observations with minimal intervention.
Moreover, it provides direct constraints on galaxy-formation scenarios, without
the systematic errors that come with the modelling of lenses.  However, the
initial tests were kept simplistic, so improvements and optimizations will have
to be made in order to have statistically significant results:
\begin{itemize}
    \item While the initial tests only used three orientations of the
    simulations, more projections should yield more matches.
    \item Line-of-sight structures, i.e. shear components (see \secref{lensing}
    for details), have been disregarded in the tests, but might be crucial for
    other lensing systems.
    \item The pollution of light from the lensing galaxy in the lensed images
    can contribute to systematic errors in the calculation of synthetic images.
    In order to avoid this problem, some process which (automatically) subtracts
    the lensing galaxy's light from the data is more ideal than the lens masking
    which was used in the tests.
    \item Stellar mass estimates derived from stellar population synthesis
    models may be compared to stellar masses of the simulations, and thus yield
    better constraints on galaxy formation scenarios.
    \item Since simulations are computed in phase-space, stellar kinematics
    provide another way of filtering through the catalogue of candidates.
    \item Although time delays are still relatively rare for galaxy lenses, they
    would enable the inference of \Ho{} and other cosmological parameters in the
    context of cosmological galaxy formation theory.
\end{itemize}
With all these improvements, plausible lens matching should present a
future-proof method to deal with the expected flood of gravitational lens
discoveries.
